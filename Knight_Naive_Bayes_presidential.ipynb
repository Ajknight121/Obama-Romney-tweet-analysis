{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "mount_file_id": "1LbBdr7SpcNoZWXdQU_q5dv28pTMUu_Ho",
      "authorship_tag": "ABX9TyMf0ngJieoBgQtAtsVlbJiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajknight121/Obama-Romney-tweet-analysis/blob/main/Knight_Naive_Bayes_presidential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Px2wcdsP9rj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean tweets\n",
        "def clean_tweet_for_new_llm(text, genericize_mentions_in_retweets=False):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_text = text\n",
        "\n",
        "    # Preserve 'RT' if it starts the tweet (case-insensitive)\n",
        "    is_retweet = original_text.strip().lower().startswith('rt')\n",
        "    retweet_prefix = 'rt ' if is_retweet else ''\n",
        "\n",
        "    # If it's a retweet, remove the 'RT' prefix from the *original_text* before further processing\n",
        "    if is_retweet:\n",
        "        text = re.sub(r'^[Rr][Tt]\\\\s*', '', original_text).strip()\n",
        "        # NEW: If it's a retweet, remove the mention that immediately follows it\n",
        "        # This targets the first @mention in the string after 'RT' is removed.\n",
        "        text = re.sub(r'^@\\\\w+\\\\s*', '', text).strip()\n",
        "    else:\n",
        "        text = original_text.strip()\n",
        "\n",
        "    # Remove HTML/XML tags like <e>...</e> or <a>...</a>\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Replace URLs with a generic link indicator\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "    text = re.sub(r'www\\\\.(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "\n",
        "    # Handle *other* mentions conditionally (the first one after RT is already handled if it was a retweet)\n",
        "    if genericize_mentions_in_retweets:\n",
        "        # Replace any remaining mentions with generic _mention_ indicator\n",
        "        text = re.sub(r'@\\\\w+', '_mention_', text)\n",
        "    else:\n",
        "        # Remove any remaining mentions\n",
        "        text = re.sub(r'@\\\\w+', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters (keeping spaces and underscore for _url_ and _mention_)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\\\s_]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "\n",
        "    # Add the retweet prefix back\n",
        "    return (retweet_prefix + text).strip()"
      ],
      "metadata": {
        "id": "VUFakN9gQFq5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Time formats for parsing\n",
        "time_formats = [\n",
        "    '%H:%M:%S%z',  # e.g., 10:02:57-05:00\n",
        "    '%I:%M:%S %p', # e.g., 1:22:46 PM\n",
        "    '%p %I:%M:%S', # e.g., PM 9:44:54\n",
        "    '%I:%M %p',    # e.g., 1:22 PM\n",
        "    '%H:%M',       # e.g., 10:02\n",
        "    '%p %I:%M'     # e.g., PM 9:44\n",
        "]\n",
        "\n",
        "def parse_mixed_time_formats(time_str):\n",
        "    if not isinstance(time_str, str):\n",
        "        return pd.NaT\n",
        "    time_str = time_str.strip() # Remove leading/trailing whitespace\n",
        "    for fmt in time_formats:\n",
        "        try:\n",
        "            return datetime.strptime(time_str, fmt).time()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT # Return NaT if no format matches"
      ],
      "metadata": {
        "id": "eTI8u5uRQIvi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function (adapted for multi-class)\n",
        "def train_model(excel_file_path):\n",
        "    # Load data\n",
        "    df_obama = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "    df_romney = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "\n",
        "\n",
        "    # Preprocess (drop first row, rename, etc.)\n",
        "    df_obama = df_obama.drop(index=0).reset_index(drop=True)\n",
        "    df_romney = df_romney.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama = df_obama.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney = df_romney.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    merged_df = pd.concat([df_obama, df_romney], ignore_index=True)\n",
        "\n",
        "    columns_to_drop = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in merged_df.columns]\n",
        "    if columns_to_drop:\n",
        "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"Excel Loaded\")\n",
        "\n",
        "    # Clean tweets\n",
        "    merged_df['tweet_clean_default'] = merged_df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "    merged_df['tweet_clean_generic_mentions'] = merged_df['Anootated tweet'].apply(lambda x: clean_tweet_for_new_llm(x, genericize_mentions_in_retweets=True))\n",
        "\n",
        "    print(\"batching sentiment inital analysis\")\n",
        "    # Batch sentiment analysis\n",
        "    get_sentiment_batch(merged_df, 'tweet_clean_default', 'sentiment_label_default', 'sentiment_score_default')\n",
        "    get_sentiment_batch(merged_df, 'tweet_clean_generic_mentions', 'sentiment_label_generic', 'sentiment_score_generic')\n",
        "\n",
        "    # Parse time\n",
        "    merged_df['date'] = pd.to_datetime(merged_df['date'], errors='coerce')\n",
        "    merged_df['time_parsed'] = merged_df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "    merged_df['hour_of_day'] = merged_df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else pd.NaT)\n",
        "\n",
        "    # Map sentiments\n",
        "    sentiment_llm_mapping = {'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0, 'ERROR': np.nan}\n",
        "    merged_df['llm_sentiment_numeric_default'] = merged_df['sentiment_label_default'].map(sentiment_llm_mapping)\n",
        "    merged_df['llm_sentiment_numeric_generic'] = merged_df['sentiment_label_generic'].map(sentiment_llm_mapping)\n",
        "\n",
        "    merged_df['llm_sentiment_numeric_default'] = merged_df['llm_sentiment_numeric_default'].fillna(0)\n",
        "    merged_df['llm_sentiment_numeric_generic'] = merged_df['llm_sentiment_numeric_generic'].fillna(0)\n",
        "    merged_df['hour_of_day'] = merged_df['hour_of_day'].fillna(0).astype(int)\n",
        "\n",
        "    print(\"DONE- batching sentiment inital analysis\")\n",
        "\n",
        "    # Include all classes: -1, 0, 1\n",
        "    valid_classes = [-1, 0, 1]\n",
        "    merged_df = merged_df[merged_df['class'].isin(valid_classes)]\n",
        "\n",
        "    # Map labels to 0,1,2 for categorical: -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
        "    label_map = {-1: 0, 0: 1, 1: 2}\n",
        "    y_mapped = merged_df['class'].map(label_map)\n",
        "    y = to_categorical(y_mapped, num_classes=3)\n",
        "\n",
        "    classes = np.unique(y_mapped)\n",
        "    class_weights = compute_class_weight('balanced', classes=classes, y=y_mapped)\n",
        "    class_weight_dict = dict(zip(classes, class_weights))\n",
        "    print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "    # TF-IDF\n",
        "    tfidf_vectorizer_default = TfidfVectorizer(max_features=10000)\n",
        "    tfidf_vectorizer_generic = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "    X_text_default = tfidf_vectorizer_default.fit_transform(merged_df['tweet_clean_default'])\n",
        "    X_text_generic = tfidf_vectorizer_generic.fit_transform(merged_df['tweet_clean_generic_mentions'])\n",
        "\n",
        "    X_numeric = merged_df[['llm_sentiment_numeric_default', 'llm_sentiment_numeric_generic', 'hour_of_day']].values\n",
        "\n",
        "    X = hstack([X_text_default, X_text_generic, X_numeric])\n",
        "    X_dense = X.toarray()\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_dense, y, test_size=0.2, random_state=42, stratify=y_mapped)\n",
        "\n",
        "    print(\"Data split training model\")\n",
        "    # Build model for multi-class\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))  # 3 classes\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Early stopping\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    # Train with class weights\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2, class_weight=class_weight_dict, callbacks=[early_stop])\n",
        "\n",
        "    print(\"DONE- Data split training model\")\n",
        "    # Evaluate\n",
        "    y_pred_proba = model.predict(X_test)\n",
        "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "    y_test_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Map back for report\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    y_test_orig = [reverse_map[label] for label in y_test_labels]\n",
        "    y_pred_orig = [reverse_map[label] for label in y_pred]\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test_orig, y_pred_orig))\n",
        "    print(classification_report(y_test_orig, y_pred_orig, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "\n",
        "    # Save model locally\n",
        "    model.save('sentiment_model.h5')\n",
        "\n",
        "    # Save vectorizers\n",
        "    joblib.dump(tfidf_vectorizer_default, 'tfidf_default.pkl')\n",
        "    joblib.dump(tfidf_vectorizer_generic, 'tfidf_generic.pkl')\n",
        "\n",
        "    # Upload to Hugging Face\n",
        "    api = HfApi()\n",
        "    api.upload_file(path_or_fileobj='sentiment_model.h5', path_in_repo='sentiment_model.h5', repo_id=hf_repo_name, repo_type=\"model\")\n",
        "    api.upload_file(path_or_fileobj='tfidf_default.pkl', path_in_repo='tfidf_default.pkl', repo_id=hf_repo_name, repo_type=\"model\")\n",
        "    api.upload_file(path_or_fileobj='tfidf_generic.pkl', path_in_repo='tfidf_generic.pkl', repo_id=hf_repo_name, repo_type=\"model\")\n",
        "\n",
        "    print(f\"Model and vectorizers saved to Hugging Face: https://huggingface.co/{hf_repo_name}\")\n",
        "\n",
        "    return model, tfidf_vectorizer_default, tfidf_vectorizer_generic"
      ],
      "metadata": {
        "id": "t-djakpmX7EI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction function (adapted for multi-class)\n",
        "def predict_on_new_data(excel_file_path, model_path='sentiment_model.h5', tfidf_default_path='tfidf_default.pkl', tfidf_generic_path='tfidf_generic.pkl'):\n",
        "    from tensorflow.keras.models import load_model\n",
        "    import joblib\n",
        "\n",
        "    # Load model and vectorizers\n",
        "    neural_network_model = load_model(model_path)\n",
        "    tfidf_vectorizer_default = joblib.load(tfidf_default_path)\n",
        "    tfidf_vectorizer_generic = joblib.load(tfidf_generic_path)\n",
        "\n",
        "    # Load new data (same as training logic)\n",
        "    try:\n",
        "        df_obama_new = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "        df_romney_new = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Excel file '{excel_file_path}' not found.\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading Excel sheets: {e}. Check sheet names.\")\n",
        "        return None\n",
        "\n",
        "    df_obama_new = df_obama_new.drop(index=0).reset_index(drop=True)\n",
        "    df_romney_new = df_romney_new.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama_new = df_obama_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney_new = df_romney_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    new_data_df = pd.concat([df_obama_new, df_romney_new], ignore_index=True)\n",
        "\n",
        "    columns_to_drop_new = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in new_data_df.columns]\n",
        "    if columns_to_drop_new:\n",
        "        new_data_df = new_data_df.drop(columns=columns_to_drop_new)\n",
        "\n",
        "    new_data_df['tweet_clean_default'] = new_data_df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "    new_data_df['tweet_clean_generic_mentions'] = new_data_df['Anootated tweet'].apply(lambda x: clean_tweet_for_new_llm(x, genericize_mentions_in_retweets=True))\n",
        "\n",
        "    # Batch sentiment analysis\n",
        "    get_sentiment_batch(new_data_df, 'tweet_clean_default', 'sentiment_label_default', 'sentiment_score_default')\n",
        "    get_sentiment_batch(new_data_df, 'tweet_clean_generic_mentions', 'sentiment_label_generic', 'sentiment_score_generic')\n",
        "\n",
        "    new_data_df['date'] = pd.to_datetime(new_data_df['date'], errors='coerce')\n",
        "    new_data_df['time_parsed'] = new_data_df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "    new_data_df['hour_of_day'] = new_data_df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else pd.NaT)\n",
        "\n",
        "    sentiment_llm_mapping = {'POSITIVE': 1, 'NEGATIVE': -1, 'NEUTRAL': 0, 'ERROR': np.nan}\n",
        "    new_data_df['llm_sentiment_numeric_default'] = new_data_df['sentiment_label_default'].map(sentiment_llm_mapping)\n",
        "    new_data_df['llm_sentiment_numeric_generic'] = new_data_df['sentiment_label_generic'].map(sentiment_llm_mapping)\n",
        "\n",
        "    new_data_df['llm_sentiment_numeric_default'] = new_data_df['llm_sentiment_numeric_default'].fillna(0)\n",
        "    new_data_df['llm_sentiment_numeric_generic'] = new_data_df['llm_sentiment_numeric_generic'].fillna(0)\n",
        "    new_data_df['hour_of_day'] = new_data_df['hour_of_day'].fillna(0).astype(int)\n",
        "\n",
        "    X_text_default_new = tfidf_vectorizer_default.transform(new_data_df['tweet_clean_default'])\n",
        "    X_text_generic_new = tfidf_vectorizer_generic.transform(new_data_df['tweet_clean_generic_mentions'])\n",
        "    X_numeric_new = new_data_df[['llm_sentiment_numeric_default', 'llm_sentiment_numeric_generic', 'hour_of_day']].values\n",
        "\n",
        "    X_new = hstack([X_text_default_new, X_text_generic_new, X_numeric_new])\n",
        "    X_new_dense = X_new.toarray()\n",
        "\n",
        "    y_pred_proba_new = neural_network_model.predict(X_new_dense)\n",
        "    y_pred = np.argmax(y_pred_proba_new, axis=1)\n",
        "\n",
        "    # Map back to original labels: 0 -> -1, 1 -> 0, 2 -> 1\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    new_data_df['pred_class'] = [reverse_map[label] for label in y_pred]\n",
        "\n",
        "    return new_data_df"
      ],
      "metadata": {
        "id": "o9XSFvdgYAK_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage - Training\n",
        "excel_file_path_train = '/content/drive/MyDrive/cs583/training-Obama-Romney-tweets.xlsx' #Training\n",
        "\n",
        "# Train and save\n",
        "model, tfidf_default, tfidf_generic = train_model(excel_file_path_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fc4f5598abf24a50bbb3631b82603637",
            "5465791b83fa46b9b27624d050085641",
            "5062e08c5e604b8da6e13d246c663cce",
            "1f86c9f3d0de4299a6b5ba0956eb58f2",
            "5d65a16f9d894f3e986fde8dc444c6e9",
            "f0f1dec0379848548707f045d30c9d62",
            "68f704e7e2d345d2a76fa8eaaf5f07c3",
            "81622c3db38b48f895e56fc40a919b15",
            "37d41b35599d4d1ba1abc1c68c14882d",
            "697eb29fd6d84d8295bb39e35c63b5a2",
            "34cff0a6cb054b1d8ba4703fbcb6bae5",
            "0de183df67594f2a882d754f02351cd2",
            "43536d6edf484bcfa32df0fb7f8ca478",
            "d68e52a67f0b476fa3ecef3958b77c75",
            "23ae893a7d924a7fb7536fc39cd02c70",
            "79a6efd0f758418bbc79b96751c28d16",
            "08e2efcc3cfe47229c5318542f270049",
            "b08ed5074f5a4f7ba3edcd70fb48cf35",
            "89aa3ae2b6a04f25b80b65d0a9593578",
            "dea9faeccfff414abe6d81c22200b5b2",
            "c858fbaaf01a408ba8633e904022cd57",
            "0ea97d068d7548b0a2767c1f3d595e9b",
            "d14a5f1eb03248d3973c25ccc079ab72",
            "f382e5d0cee74b35b5282a637671975c",
            "546c5f5126654e16bd05eb549068be92",
            "e2cf153fab964db5aa830042824118f7",
            "dadddbf706284aa887d8858caf2955e6",
            "7700a29d05984ca5a82b13343a0fd1a3",
            "7bf582c082334cce89253ba282143be9",
            "31beb61944e44cfc94c07b3aa155d3c2",
            "37923f01ff7f47c88206280d01f1add5",
            "1c6a7119fe994bc4bdc9473bc203f28b",
            "de3b0e1eef7f47dd966cfa80e1f88adb",
            "48930374d0594535be2ec24c95ec9f88",
            "223032f90fa64eb28627074df616bc7d",
            "e415296b702545218850336129fcf154",
            "83ab1b339ed443e59510ca824adee16f",
            "0f5dde1a904c4b168b0d00102e8b0892",
            "b2a22b04336042918568500ac3b652ba",
            "31f85cedf3db4ea8bac5a9d8a2c02780",
            "0816b9ab8e1e46ac9957c72f701c8506",
            "1dfbbca4b69047e1b9d23d691946c040",
            "c15f208bdf564f5889cc2001a3d95d0e",
            "dc5ce0c9c2c14119b681dfb8312e08dd",
            "93e816247c1a42cb8f162c9efc303f55",
            "4f98d1cfd95c48deac72868d7e58230e",
            "15a279c90c8845b3b10a9cc9270aa771",
            "31983994f37c48b093fc0499946e6769",
            "39cce549083d45c6902bf91531fe4608",
            "36056eaf2eec4050b6891a4df1b0f232",
            "d26b296c68244bd7b636c52338c70aa4",
            "8ef3da93fb314b34b49f5ce7e2793f43",
            "f671550697f046f4a9ddd9265b2d1fbb",
            "4a6d8310bb8f4db092d314390c962499",
            "d6440e2d95344e7094199680c871a619",
            "7b480f61b5c34af0b29881d921e22e55",
            "2e55335b50794fb383944890212f1e38",
            "7f5a8c48dc084ce0bd92165197d137e8",
            "a12e89615db441a9b9a3a3de8060b239",
            "a137ac1e93d0493ab530afe2e16b7e37",
            "c3c742b1cad64f499fd1e076f0a0beda",
            "b85b27845e754681ad2ce3e57fe01bf2",
            "706138a345134f5c8dfb3b06b0eecd52",
            "ed87f9512dd94639b3d39ca56bd16679",
            "09e5abdef9a94e19a3cc1d3faf844b37",
            "b6170430d9d2410ebc93094b0dcabd4e",
            "ad7568e7f1ed4a50938bf9e02a8910d9",
            "87bb4e6328fa413e98c1b7484ed285cb",
            "021630ff79c64362b66a52d5a4ec6377",
            "1636b497cfe94adf8550f9995bb45607",
            "2459e915f0434c85846d69c028c97e11",
            "7c7b75b7acf04ff09bbdaf2d6397412f",
            "e6b1b651450d4874964920bb9927533e",
            "da9089f19f6942d09a8d1ddb5ac56e34",
            "62f286f840844c9786b91b24049a3b29",
            "0f8c5c57dbac42418524664df4a90f39",
            "59dfa2214a724610885e9d192d4f4ad8",
            "91a7aefe16b9457a8c59b088fc8d5a9d",
            "bf342e74964d440580866d989141071f",
            "9c302d3622bd4e3a88043cd67d07d7e1",
            "32af4d3e4216493984eb8bb66e5da101",
            "2c33c782117c4d7fb12bfdbff1703d90",
            "fbb9021606184f4ab334da63ff3d2b37",
            "a2d4927f601e4abd87293f7d8948bdde",
            "0ec744b5bf274f1c92fb269f25a612b2",
            "13729836d4d949edbacc6c01119ad9dd",
            "050c22f3de4248ea93a4ca892476ddfa",
            "a6bee2fe868b444a945dcda4c83ae96b",
            "db236cbda02441fe870ce7f478d7d303",
            "4287ac7f5b9f4feab32159bafaf55a0e",
            "860d37a498654f1a96a39e565023a68c",
            "1a8a46e520044ebb923bcbb73b801188",
            "6003ca67d08048669068c672a96477d7",
            "633fb4a255a34f4281564179b5d7fd14",
            "40cf736bc8a1482aae75df472459234b",
            "2fcd83cecceb4d6994ebc3e127147147",
            "878180872e254350a5dd9dc1c9919a4c",
            "bfa6b9bede13435cade01117e1e24285",
            "d51d6917e13e4bbb87501596a6504b2f"
          ]
        },
        "id": "scRuA_kWYXf9",
        "outputId": "0b6bcd95-6d0f-4473-c1f4-236412e9412d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel Loaded\n",
            "batching sentiment inital analysis\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1678167856.py:44: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  merged_df['hour_of_day'] = merged_df['hour_of_day'].fillna(0).astype(int)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE- batching sentiment inital analysis\n",
            "Class weights: {np.int64(0): np.float64(0.7697473174108688), np.int64(1): np.float64(1.0364466815809097), np.int64(2): np.float64(1.3586265884652982)}\n",
            "Data split training model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 35ms/step - accuracy: 0.3226 - loss: 1.1088 - val_accuracy: 0.3401 - val_loss: 1.0972\n",
            "Epoch 2/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3719 - loss: 1.0917 - val_accuracy: 0.3856 - val_loss: 1.0916\n",
            "Epoch 3/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3778 - loss: 1.0930 - val_accuracy: 0.3997 - val_loss: 1.0835\n",
            "Epoch 4/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4586 - loss: 1.0494 - val_accuracy: 0.3716 - val_loss: 1.0843\n",
            "Epoch 5/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6108 - loss: 0.9317 - val_accuracy: 0.3687 - val_loss: 1.0934\n",
            "Epoch 6/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7996 - loss: 0.5855 - val_accuracy: 0.3418 - val_loss: 1.1237\n",
            "Epoch 7/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8969 - loss: 0.3230 - val_accuracy: 0.3328 - val_loss: 1.1651\n",
            "Epoch 8/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9262 - loss: 0.2320 - val_accuracy: 0.3468 - val_loss: 1.1145\n",
            "Epoch 9/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9257 - loss: 0.2072 - val_accuracy: 0.3564 - val_loss: 1.1201\n",
            "Epoch 10/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9319 - loss: 0.1905 - val_accuracy: 0.3198 - val_loss: 1.1854\n",
            "Epoch 11/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9371 - loss: 0.1670 - val_accuracy: 0.3198 - val_loss: 1.2009\n",
            "Epoch 12/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9391 - loss: 0.1660 - val_accuracy: 0.3361 - val_loss: 1.2076\n",
            "Epoch 13/50\n",
            "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9333 - loss: 0.1608 - val_accuracy: 0.3007 - val_loss: 1.2141\n",
            "DONE- Data split training model\n",
            "\u001b[1m70/70\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4087230215827338\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.50      0.37      0.43       963\n",
            "  Neutral (0)       0.38      0.55      0.45       715\n",
            " Positive (1)       0.33      0.30      0.31       546\n",
            "\n",
            "     accuracy                           0.41      2224\n",
            "    macro avg       0.41      0.40      0.40      2224\n",
            " weighted avg       0.42      0.41      0.41      2224\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc4f5598abf24a50bbb3631b82603637"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0de183df67594f2a882d754f02351cd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  sentiment_model.h5          :   4%|3         | 1.12MB / 30.9MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d14a5f1eb03248d3973c25ccc079ab72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48930374d0594535be2ec24c95ec9f88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93e816247c1a42cb8f162c9efc303f55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  tfidf_default.pkl           :  59%|#####9    |  616kB / 1.04MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b480f61b5c34af0b29881d921e22e55"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad7568e7f1ed4a50938bf9e02a8910d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91a7aefe16b9457a8c59b088fc8d5a9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  tfidf_generic.pkl           : 100%|##########| 1.04MB / 1.04MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db236cbda02441fe870ce7f478d7d303"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and vectorizers saved to Hugging Face: https://huggingface.co/Ajknight/obama-romney-sentiment-model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "new_cell_1",
        "outputId": "8eaf05f1-ba89-4193-a637-3c29712cc51b"
      },
      "source": [
        "# Usage - Prediction\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "excel_file_path_pred = '/content/drive/MyDrive/cs583/training-Obama-Romney-tweets.xlsx' #Testing\n",
        "\n",
        "# Predict\n",
        "predicted_df = predict_on_new_data(excel_file_path_pred)\n",
        "\n",
        "if predicted_df is not None:\n",
        "    print(\"Predictions on new data generated successfully. Head of predicted_df:\")\n",
        "    print(predicted_df[['Anootated tweet', 'pred_class']].head())\n",
        "\n",
        "    # Calculate and print Accuracy and F-score\n",
        "    y_true = predicted_df['class']\n",
        "    y_pred = predicted_df['pred_class']\n",
        "\n",
        "    # Ensure both are numeric and handle potential non-numeric values if any slipped through\n",
        "    y_true = pd.to_numeric(y_true, errors='coerce').dropna()\n",
        "    y_pred = pd.to_numeric(y_pred, errors='coerce').dropna()\n",
        "\n",
        "    # Align indices after dropping NaNs\n",
        "    common_indices = y_true.index.intersection(y_pred.index)\n",
        "    y_true = y_true.loc[common_indices]\n",
        "    y_pred = y_pred.loc[common_indices]\n",
        "\n",
        "    if not y_true.empty and not y_pred.empty:\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        report = classification_report(y_true, y_pred, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'], zero_division=0)\n",
        "\n",
        "        print(\"\\n--- Prediction Metrics ---\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "    else:\n",
        "        print(\"\\nWarning: Not enough valid data points to calculate metrics.\")\n",
        "\n",
        "    # Generate the .txt output file\n",
        "    output_filename = 'predictions.txt'\n",
        "    with open(output_filename, 'w') as f:\n",
        "        f.write('(setf x *(\\n')\n",
        "        for original_index, row in predicted_df.iterrows():\n",
        "            # The tweet number here must correspond to the line/tweet number in each Excel file.\n",
        "            # We use original_index + 1 to get 1-based indexing for tweet number\n",
        "            f.write(f'  ({original_index + 1} {row[\"pred_class\"]})\\n')\n",
        "        f.write(') )\\n')\n",
        "    print(f\"Predictions saved to {output_filename} in the specified format.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'predict_on_new_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4076791738.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpredicted_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_on_new_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexcel_file_path_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpredicted_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'predict_on_new_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "6wyMpdysS_sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import joblib\n",
        "\n",
        "# Function to clean tweets (from the notebook)\n",
        "def clean_tweet_for_new_llm(text, genericize_mentions_in_retweets=False):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_text = text\n",
        "\n",
        "    # Preserve 'RT' if it starts the tweet (case-insensitive)\n",
        "    is_retweet = original_text.strip().lower().startswith('rt')\n",
        "    retweet_prefix = 'rt ' if is_retweet else ''\n",
        "\n",
        "    # If it's a retweet, remove the 'RT' prefix from the *original_text* before further processing\n",
        "    if is_retweet:\n",
        "        text = re.sub(r'^[Rr][Tt]\\\\s*', '', original_text).strip()\n",
        "        # NEW: If it's a retweet, remove the mention that immediately follows it\n",
        "        # This targets the first @mention in the string after 'RT' is removed.\n",
        "        text = re.sub(r'^@\\\\w+\\\\s*', '', text).strip()\n",
        "    else:\n",
        "        text = original_text.strip()\n",
        "\n",
        "    # Remove HTML/XML tags like <e>...</e> or <a>...</a>\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Replace URLs with a generic link indicator\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "    text = re.sub(r'www\\\\.(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "\n",
        "    # Handle *other* mentions conditionally (the first one after RT is already handled if it was a retweet)\n",
        "    if genericize_mentions_in_retweets:\n",
        "        # Replace any remaining mentions with generic _mention_ indicator\n",
        "        text = re.sub(r'@\\\\w+', '_mention_', text)\n",
        "    else:\n",
        "        # Remove any remaining mentions\n",
        "        text = re.sub(r'@\\\\w+', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters (keeping spaces and underscore for _url_ and _mention_)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\\\s_]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "\n",
        "    # Add the retweet prefix back\n",
        "    return (retweet_prefix + text).strip()\n",
        "\n",
        "# Time formats for parsing (from the notebook)\n",
        "time_formats = [\n",
        "    '%H:%M:%S%z',  # e.g., 10:02:57-05:00\n",
        "    '%I:%M:%S %p', # e.g., 1:22:46 PM\n",
        "    '%p %I:%M:%S', # e.g., PM 9:44:54\n",
        "    '%I:%M %p',    # e.g., 1:22 PM\n",
        "    '%H:%M',       # e.g., 10:02\n",
        "    '%p %I:%M'     # e.g., PM 9:44\n",
        "]\n",
        "\n",
        "def parse_mixed_time_formats(time_str):\n",
        "    if not isinstance(time_str, str):\n",
        "        return pd.NaT\n",
        "    time_str = time_str.strip() # Remove leading/trailing whitespace\n",
        "    for fmt in time_formats:\n",
        "        try:\n",
        "            return datetime.strptime(time_str, fmt).time()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT # Return NaT if no format matches\n",
        "\n",
        "# Modified train function for Naive Bayes\n",
        "def train_models(excel_file_path):\n",
        "    # Load data\n",
        "    df_obama = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "    df_romney = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "\n",
        "    df_obama = df_obama.drop(index=0).reset_index(drop=True)\n",
        "    df_romney = df_romney.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama = df_obama.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney = df_romney.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    merged_df = pd.concat([df_obama, df_romney], ignore_index=True)\n",
        "\n",
        "    columns_to_drop = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in merged_df.columns]\n",
        "    if columns_to_drop:\n",
        "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"Excel Loaded\")\n",
        "\n",
        "    # Clean tweets (using default, without generic mentions for simplicity)\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['tweet_clean'] = df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "\n",
        "    # Parse time\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        df['time_parsed'] = df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "        df['hour_of_day'] = df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "        df['hour_of_day'] = df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Include all classes: -1, 0, 1\n",
        "    valid_classes = [-1, 0, 1]\n",
        "    df_obama = df_obama[df_obama['class'].isin(valid_classes)]\n",
        "    df_romney = df_romney[df_romney['class'].isin(valid_classes)]\n",
        "    merged_df = merged_df[merged_df['class'].isin(valid_classes)]\n",
        "\n",
        "    # Map labels to 0,1,2 for categorical: -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
        "    label_map = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "    # Function to prepare X and fit\n",
        "    def prepare_and_fit(df, prefix):\n",
        "        tfidf = TfidfVectorizer(max_features=10000)\n",
        "        X_text = tfidf.fit_transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_columns = hour_onehot.columns\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "\n",
        "        y = df['class'].map(label_map)\n",
        "\n",
        "        model = MultinomialNB()\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Save\n",
        "        joblib.dump(model, f'nb_{prefix}.pkl')\n",
        "        joblib.dump(tfidf, f'tfidf_{prefix}.pkl')\n",
        "        joblib.dump(hour_columns, f'hour_columns_{prefix}.pkl')\n",
        "\n",
        "        return model, tfidf, hour_columns\n",
        "\n",
        "    # Train on Obama\n",
        "    model_obama, tfidf_obama, hour_columns_obama = prepare_and_fit(df_obama, 'obama')\n",
        "\n",
        "    # Train on Romney\n",
        "    model_romney, tfidf_romney, hour_columns_romney = prepare_and_fit(df_romney, 'romney')\n",
        "\n",
        "    # Train on merged\n",
        "    model_merged, tfidf_merged, hour_columns_merged = prepare_and_fit(merged_df, 'merged')\n",
        "\n",
        "    # Evaluate on split for merged as example\n",
        "    X_merged = hstack([tfidf_merged.transform(merged_df['tweet_clean']),\n",
        "                       csr_matrix(pd.get_dummies(merged_df['hour_of_day'], prefix='hour').values)])\n",
        "    y_merged = merged_df['class'].map(label_map)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_merged, y_merged, test_size=0.2, random_state=42, stratify=y_merged)\n",
        "\n",
        "    y_pred = model_merged.predict(X_test)\n",
        "    y_test_orig = [label_map.get(label, label) for label in y_test]  # Wait, reverse\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    y_test_orig = [reverse_map[label] for label in y_test]\n",
        "    y_pred_orig = [reverse_map[label] for label in y_pred]\n",
        "\n",
        "    print(\"Accuracy on merged test split:\", accuracy_score(y_test_orig, y_pred_orig))\n",
        "    print(classification_report(y_test_orig, y_pred_orig, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "\n",
        "    print(\"Models trained and saved.\")\n",
        "\n",
        "    return model_obama, tfidf_obama, hour_columns_obama, model_romney, tfidf_romney, hour_columns_romney, model_merged, tfidf_merged, hour_columns_merged\n",
        "\n",
        "# Modified prediction function\n",
        "def predict_on_new_data(excel_file_path):\n",
        "    # Load models, tfidfs, hour_columns\n",
        "    model_obama = joblib.load('nb_obama.pkl')\n",
        "    tfidf_obama = joblib.load('tfidf_obama.pkl')\n",
        "    hour_columns_obama = joblib.load('hour_columns_obama.pkl')\n",
        "\n",
        "    model_romney = joblib.load('nb_romney.pkl')\n",
        "    tfidf_romney = joblib.load('tfidf_romney.pkl')\n",
        "    hour_columns_romney = joblib.load('hour_columns_romney.pkl')\n",
        "\n",
        "    model_merged = joblib.load('nb_merged.pkl')\n",
        "    tfidf_merged = joblib.load('tfidf_merged.pkl')\n",
        "    hour_columns_merged = joblib.load('hour_columns_merged.pkl')\n",
        "\n",
        "    # Load new data\n",
        "    try:\n",
        "        df_obama_new = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "        df_romney_new = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Excel file '{excel_file_path}' not found.\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading Excel sheets: {e}. Check sheet names.\")\n",
        "        return None\n",
        "\n",
        "    df_obama_new = df_obama_new.drop(index=0).reset_index(drop=True)\n",
        "    df_romney_new = df_romney_new.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama_new = df_obama_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney_new = df_romney_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    new_data_df = pd.concat([df_obama_new, df_romney_new], ignore_index=True)\n",
        "\n",
        "    columns_to_drop_new = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in new_data_df.columns]\n",
        "    if columns_to_drop_new:\n",
        "        new_data_df = new_data_df.drop(columns=columns_to_drop_new)\n",
        "\n",
        "    new_data_df['tweet_clean'] = new_data_df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "\n",
        "    new_data_df['date'] = pd.to_datetime(new_data_df['date'], errors='coerce')\n",
        "    new_data_df['time_parsed'] = new_data_df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "    new_data_df['hour_of_day'] = new_data_df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "    new_data_df['hour_of_day'] = new_data_df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Function to prepare X for prediction\n",
        "    def prepare_X_new(df, tfidf, hour_columns):\n",
        "        X_text = tfidf.transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_onehot = hour_onehot.reindex(columns=hour_columns, fill_value=0)\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "        return X\n",
        "\n",
        "    # Get probas\n",
        "    X_obama_new = prepare_X_new(new_data_df, tfidf_obama, hour_columns_obama)\n",
        "    proba_obama = model_obama.predict_proba(X_obama_new)\n",
        "    class_obama = np.argmax(proba_obama, axis=1)\n",
        "    max_proba_obama = np.max(proba_obama, axis=1)\n",
        "\n",
        "    X_romney_new = prepare_X_new(new_data_df, tfidf_romney, hour_columns_romney)\n",
        "    proba_romney = model_romney.predict_proba(X_romney_new)\n",
        "    class_romney = np.argmax(proba_romney, axis=1)\n",
        "    max_proba_romney = np.max(proba_romney, axis=1)\n",
        "\n",
        "    X_merged_new = prepare_X_new(new_data_df, tfidf_merged, hour_columns_merged)\n",
        "    proba_merged = model_merged.predict_proba(X_merged_new)\n",
        "    class_merged = np.argmax(proba_merged, axis=1)\n",
        "    max_proba_merged = np.max(proba_merged, axis=1)\n",
        "\n",
        "    # Select the class with the highest probability across models\n",
        "    pred_classes = []\n",
        "    for i in range(len(new_data_df)):\n",
        "        probas = [max_proba_obama[i], max_proba_romney[i], max_proba_merged[i]]\n",
        "        classes = [class_obama[i], class_romney[i], class_merged[i]]\n",
        "        max_idx = np.argmax(probas)\n",
        "        pred_classes.append(classes[max_idx])\n",
        "\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    new_data_df['pred_class'] = [reverse_map[c] for c in pred_classes]\n",
        "\n",
        "    return new_data_df\n",
        "\n",
        "# Usage example (adapt paths as needed)\n",
        "excel_file_path = '/content/drive/MyDrive/cs583/training-Obama-Romney-tweets.xlsx'\n",
        "train_models(excel_file_path)\n",
        "predicted_df = predict_on_new_data(excel_file_path)\n",
        "if predicted_df is not None:\n",
        "    print(predicted_df[['Anootated tweet', 'pred_class']].head())\n",
        "    # Metrics if 'class' is available\n",
        "    y_true = pd.to_numeric(predicted_df['class'], errors='coerce').dropna()\n",
        "    y_pred = pd.to_numeric(predicted_df['pred_class'], errors='coerce').dropna()\n",
        "    # ... calculate accuracy etc.\n",
        "\n",
        "# Generate the .txt output file\n",
        "output_filename = 'predictionsBayes.txt'\n",
        "with open(output_filename, 'w') as f:\n",
        "    f.write('(setf x *(\\n')\n",
        "    for original_index, row in predicted_df.iterrows():\n",
        "        # The tweet number here must correspond to the line/tweet number in each Excel file.\n",
        "        # We use original_index + 1 to get 1-based indexing for tweet number\n",
        "        f.write(f'  ({original_index + 1} {row[\"pred_class\"]})\\n')\n",
        "    f.write(') )\\n')\n",
        "print(f\"Predictions saved to {output_filename} in the specified format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zZyHiFUS9iF",
        "outputId": "22096a8b-5fb0-41be-8197-e71d7d2b3536"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel Loaded\n",
            "Accuracy on merged test split: 0.8080035971223022\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.73      0.99      0.84       963\n",
            "  Neutral (0)       0.89      0.81      0.85       715\n",
            " Positive (1)       0.99      0.48      0.65       546\n",
            "\n",
            "     accuracy                           0.81      2224\n",
            "    macro avg       0.87      0.76      0.78      2224\n",
            " weighted avg       0.84      0.81      0.80      2224\n",
            "\n",
            "Models trained and saved.\n",
            "                                     Anootated tweet  pred_class\n",
            "0  Kirkpatrick, who wore a baseball cap embroider...          -1\n",
            "1  Question: If <e>Romney</e> and <e>Obama</e> ha...          -1\n",
            "2  #<e>obama</e> debates that Cracker Ass Cracker...          -1\n",
            "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...          -1\n",
            "4  @Hollivan @hereistheanswer  Youre missing the ...          -1\n",
            "Predictions saved to predictionsBayes.txt in the specified format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Steming Naive Bayes"
      ],
      "metadata": {
        "id": "HvKZi4hKWqgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import joblib\n",
        "from nltk.stem import PorterStemmer  # Added for stemming\n",
        "import nltk  # Added for stemming\n",
        "nltk.download('punkt', quiet=True)  # Download if needed, quietly\n",
        "\n",
        "# Function to clean tweets (from the notebook), with optional stemming\n",
        "def clean_tweet_for_new_llm(text, genericize_mentions_in_retweets=False, use_stemming=False):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_text = text\n",
        "\n",
        "    # Preserve 'RT' if it starts the tweet (case-insensitive)\n",
        "    is_retweet = original_text.strip().lower().startswith('rt')\n",
        "    retweet_prefix = 'rt ' if is_retweet else ''\n",
        "\n",
        "    # If it's a retweet, remove the 'RT' prefix from the *original_text* before further processing\n",
        "    if is_retweet:\n",
        "        text = re.sub(r'^[Rr][Tt]\\\\s*', '', original_text).strip()\n",
        "        # NEW: If it's a retweet, remove the mention that immediately follows it\n",
        "        # This targets the first @mention in the string after 'RT' is removed.\n",
        "        text = re.sub(r'^@\\\\w+\\\\s*', '', text).strip()\n",
        "    else:\n",
        "        text = original_text.strip()\n",
        "\n",
        "    # Remove HTML/XML tags like <e>...</e> or <a>...</a>\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Replace URLs with a generic link indicator\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "    text = re.sub(r'www\\\\.(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "\n",
        "    # Handle *other* mentions conditionally (the first one after RT is already handled if it was a retweet)\n",
        "    if genericize_mentions_in_retweets:\n",
        "        # Replace any remaining mentions with generic _mention_ indicator\n",
        "        text = re.sub(r'@\\\\w+', '_mention_', text)\n",
        "    else:\n",
        "        # Remove any remaining mentions\n",
        "        text = re.sub(r'@\\\\w+', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters (keeping spaces and underscore for _url_ and _mention_)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\\\s_]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Optional stemming: Split into words, stem, and rejoin\n",
        "    if use_stemming:\n",
        "        stemmer = PorterStemmer()\n",
        "        words = text.split()\n",
        "        stemmed_words = [stemmer.stem(word) for word in words]\n",
        "        text = ' '.join(stemmed_words)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "\n",
        "    # Add the retweet prefix back\n",
        "    return (retweet_prefix + text).strip()\n",
        "\n",
        "# Time formats for parsing (from the notebook)\n",
        "time_formats = [\n",
        "    '%H:%M:%S%z',  # e.g., 10:02:57-05:00\n",
        "    '%I:%M:%S %p', # e.g., 1:22:46 PM\n",
        "    '%p %I:%M:%S', # e.g., PM 9:44:54\n",
        "    '%I:%M %p',    # e.g., 1:22 PM\n",
        "    '%H:%M',       # e.g., 10:02\n",
        "    '%p %I:%M'     # e.g., PM 9:44\n",
        "]\n",
        "\n",
        "def parse_mixed_time_formats(time_str):\n",
        "    if not isinstance(time_str, str):\n",
        "        return pd.NaT\n",
        "    time_str = time_str.strip() # Remove leading/trailing whitespace\n",
        "    for fmt in time_formats:\n",
        "        try:\n",
        "            return datetime.strptime(time_str, fmt).time()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT # Return NaT if no format matches\n",
        "\n",
        "# Modified train function for Naive Bayes\n",
        "def train_models(excel_file_path):\n",
        "    # Load data\n",
        "    df_obama = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "    df_romney = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "\n",
        "    df_obama = df_obama.drop(index=0).reset_index(drop=True)\n",
        "    df_romney = df_romney.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama = df_obama.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney = df_romney.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    merged_df = pd.concat([df_obama, df_romney], ignore_index=True)\n",
        "\n",
        "    columns_to_drop = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in merged_df.columns]\n",
        "    if columns_to_drop:\n",
        "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"Excel Loaded\")\n",
        "\n",
        "    # Clean tweets (using default, without generic mentions for simplicity; enable stemming if desired)\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['tweet_clean'] = df['Anootated tweet'].apply(lambda x: clean_tweet_for_new_llm(x, use_stemming=True))  # Example: Enable stemming here\n",
        "\n",
        "    # Parse time\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        df['time_parsed'] = df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "        df['hour_of_day'] = df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "        df['hour_of_day'] = df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Include all classes: -1, 0, 1\n",
        "    valid_classes = [-1, 0, 1]\n",
        "    df_obama = df_obama[df_obama['class'].isin(valid_classes)]\n",
        "    df_romney = df_romney[df_romney['class'].isin(valid_classes)]\n",
        "    merged_df = merged_df[merged_df['class'].isin(valid_classes)]\n",
        "\n",
        "    # Map labels to 0,1,2 for categorical: -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
        "    label_map = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "    # Function to prepare X and fit\n",
        "    def prepare_and_fit(df, prefix):\n",
        "        tfidf = TfidfVectorizer(max_features=10000)\n",
        "        X_text = tfidf.fit_transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_columns = hour_onehot.columns\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "\n",
        "        y = df['class'].map(label_map)\n",
        "\n",
        "        model = MultinomialNB()\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Save\n",
        "        joblib.dump(model, f'nb_{prefix}.pkl')\n",
        "        joblib.dump(tfidf, f'tfidf_{prefix}.pkl')\n",
        "        joblib.dump(hour_columns, f'hour_columns_{prefix}.pkl')\n",
        "\n",
        "        return model, tfidf, hour_columns\n",
        "\n",
        "    # Train on Obama\n",
        "    model_obama, tfidf_obama, hour_columns_obama = prepare_and_fit(df_obama, 'obama')\n",
        "\n",
        "    # Train on Romney\n",
        "    model_romney, tfidf_romney, hour_columns_romney = prepare_and_fit(df_romney, 'romney')\n",
        "\n",
        "    # Train on merged\n",
        "    model_merged, tfidf_merged, hour_columns_merged = prepare_and_fit(merged_df, 'merged')\n",
        "\n",
        "    # Evaluate on split for merged as example\n",
        "    X_merged = hstack([tfidf_merged.transform(merged_df['tweet_clean']),\n",
        "                       csr_matrix(pd.get_dummies(merged_df['hour_of_day'], prefix='hour').values)])\n",
        "    y_merged = merged_df['class'].map(label_map)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_merged, y_merged, test_size=0.2, random_state=42, stratify=y_merged)\n",
        "\n",
        "    y_pred = model_merged.predict(X_test)\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    y_test_orig = [reverse_map[label] for label in y_test]\n",
        "    y_pred_orig = [reverse_map[label] for label in y_pred]\n",
        "\n",
        "    print(\"Accuracy on merged test split:\", accuracy_score(y_test_orig, y_pred_orig))\n",
        "    print(classification_report(y_test_orig, y_pred_orig, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "\n",
        "    print(\"Models trained and saved.\")\n",
        "\n",
        "    return model_obama, tfidf_obama, hour_columns_obama, model_romney, tfidf_romney, hour_columns_romney, model_merged, tfidf_merged, hour_columns_merged\n",
        "\n",
        "# Modified prediction function\n",
        "def predict_on_new_data(excel_file_path):\n",
        "    # Load models, tfidfs, hour_columns\n",
        "    model_obama = joblib.load('nb_obama.pkl')\n",
        "    tfidf_obama = joblib.load('tfidf_obama.pkl')\n",
        "    hour_columns_obama = joblib.load('hour_columns_obama.pkl')\n",
        "\n",
        "    model_romney = joblib.load('nb_romney.pkl')\n",
        "    tfidf_romney = joblib.load('tfidf_romney.pkl')\n",
        "    hour_columns_romney = joblib.load('hour_columns_romney.pkl')\n",
        "\n",
        "    model_merged = joblib.load('nb_merged.pkl')\n",
        "    tfidf_merged = joblib.load('tfidf_merged.pkl')\n",
        "    hour_columns_merged = joblib.load('hour_columns_merged.pkl')\n",
        "\n",
        "    # Load new data\n",
        "    try:\n",
        "        df_obama_new = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "        df_romney_new = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Excel file '{excel_file_path}' not found.\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading Excel sheets: {e}. Check sheet names.\")\n",
        "        return None\n",
        "\n",
        "    df_obama_new = df_obama_new.drop(index=0).reset_index(drop=True)\n",
        "    df_romney_new = df_romney_new.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama_new = df_obama_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney_new = df_romney_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    new_data_df = pd.concat([df_obama_new, df_romney_new], ignore_index=True)\n",
        "\n",
        "    columns_to_drop_new = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in new_data_df.columns]\n",
        "    if columns_to_drop_new:\n",
        "        new_data_df = new_data_df.drop(columns=columns_to_drop_new)\n",
        "\n",
        "    new_data_df['tweet_clean'] = new_data_df['Anootated tweet'].apply(lambda x: clean_tweet_for_new_llm(x, use_stemming=True))  # Example: Enable stemming here\n",
        "\n",
        "    new_data_df['date'] = pd.to_datetime(new_data_df['date'], errors='coerce')\n",
        "    new_data_df['time_parsed'] = new_data_df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "    new_data_df['hour_of_day'] = new_data_df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "    new_data_df['hour_of_day'] = new_data_df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Function to prepare X for prediction\n",
        "    def prepare_X_new(df, tfidf, hour_columns):\n",
        "        X_text = tfidf.transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_onehot = hour_onehot.reindex(columns=hour_columns, fill_value=0)\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "        return X\n",
        "\n",
        "    # Get probas\n",
        "    X_obama_new = prepare_X_new(new_data_df, tfidf_obama, hour_columns_obama)\n",
        "    proba_obama = model_obama.predict_proba(X_obama_new)\n",
        "    class_obama = np.argmax(proba_obama, axis=1)\n",
        "    max_proba_obama = np.max(proba_obama, axis=1)\n",
        "\n",
        "    X_romney_new = prepare_X_new(new_data_df, tfidf_romney, hour_columns_romney)\n",
        "    proba_romney = model_romney.predict_proba(X_romney_new)\n",
        "    class_romney = np.argmax(proba_romney, axis=1)\n",
        "    max_proba_romney = np.max(proba_romney, axis=1)\n",
        "\n",
        "    X_merged_new = prepare_X_new(new_data_df, tfidf_merged, hour_columns_merged)\n",
        "    proba_merged = model_merged.predict_proba(X_merged_new)\n",
        "    class_merged = np.argmax(proba_merged, axis=1)\n",
        "    max_proba_merged = np.max(proba_merged, axis=1)\n",
        "\n",
        "    # Select the class with the highest probability across models\n",
        "    pred_classes = []\n",
        "    for i in range(len(new_data_df)):\n",
        "        probas = [max_proba_obama[i], max_proba_romney[i], max_proba_merged[i]]\n",
        "        classes = [class_obama[i], class_romney[i], class_merged[i]]\n",
        "        max_idx = np.argmax(probas)\n",
        "        pred_classes.append(classes[max_idx])\n",
        "\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    new_data_df['pred_class'] = [reverse_map[c] for c in pred_classes]\n",
        "\n",
        "    return new_data_df\n",
        "\n",
        "# Usage example (adapt paths as needed)\n",
        "excel_file_path = '/content/drive/MyDrive/cs583/training-Obama-Romney-tweets.xlsx'\n",
        "train_models(excel_file_path)\n",
        "predicted_df = predict_on_new_data(excel_file_path)\n",
        "if predicted_df is not None:\n",
        "    print(predicted_df[['Anootated tweet', 'pred_class']].head())\n",
        "    # Metrics if 'class' is available\n",
        "    y_true = pd.to_numeric(predicted_df['class'], errors='coerce').dropna()\n",
        "    y_pred = pd.to_numeric(predicted_df['pred_class'], errors='coerce').dropna()\n",
        "    # ... calculate accuracy etc.\n",
        "\n",
        "# Generate the .txt output file\n",
        "output_filename = 'predictionsBayesStem.txt'\n",
        "with open(output_filename, 'w') as f:\n",
        "    f.write('(setf x *(\\n')\n",
        "    for original_index, row in predicted_df.iterrows():\n",
        "        # The tweet number here must correspond to the line/tweet number in each Excel file.\n",
        "        # We use original_index + 1 to get 1-based indexing for tweet number\n",
        "        f.write(f'  ({original_index + 1} {row[\"pred_class\"]})\\n')\n",
        "    f.write(') )\\n')\n",
        "print(f\"Predictions saved to {output_filename} in the specified format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRqmymsoWDjH",
        "outputId": "88f74203-bf99-440a-f1c4-389378dbabda"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel Loaded\n",
            "Accuracy on merged test split: 0.8080035971223022\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.73      0.99      0.84       963\n",
            "  Neutral (0)       0.89      0.81      0.85       715\n",
            " Positive (1)       0.99      0.48      0.65       546\n",
            "\n",
            "     accuracy                           0.81      2224\n",
            "    macro avg       0.87      0.76      0.78      2224\n",
            " weighted avg       0.84      0.81      0.80      2224\n",
            "\n",
            "Models trained and saved.\n",
            "                                     Anootated tweet  pred_class\n",
            "0  Kirkpatrick, who wore a baseball cap embroider...          -1\n",
            "1  Question: If <e>Romney</e> and <e>Obama</e> ha...          -1\n",
            "2  #<e>obama</e> debates that Cracker Ass Cracker...          -1\n",
            "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...          -1\n",
            "4  @Hollivan @hereistheanswer  Youre missing the ...          -1\n",
            "Predictions saved to predictionsBayes.txt in the specified format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results:\n",
        "Stemming did not improve the accuracy of naive bayes model"
      ],
      "metadata": {
        "id": "INTCKTcKW_cd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM model"
      ],
      "metadata": {
        "id": "UbDFDHVZZm4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import joblib\n",
        "\n",
        "# Function to clean tweets (from the notebook)\n",
        "def clean_tweet_for_new_llm(text, genericize_mentions_in_retweets=False):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    original_text = text\n",
        "\n",
        "    # Preserve 'RT' if it starts the tweet (case-insensitive)\n",
        "    is_retweet = original_text.strip().lower().startswith('rt')\n",
        "    retweet_prefix = 'rt ' if is_retweet else ''\n",
        "\n",
        "    # If it's a retweet, remove the 'RT' prefix from the *original_text* before further processing\n",
        "    if is_retweet:\n",
        "        text = re.sub(r'^[Rr][Tt]\\\\s*', '', original_text).strip()\n",
        "        # NEW: If it's a retweet, remove the mention that immediately follows it\n",
        "        # This targets the first @mention in the string after 'RT' is removed.\n",
        "        text = re.sub(r'^@\\\\w+\\\\s*', '', text).strip()\n",
        "    else:\n",
        "        text = original_text.strip()\n",
        "\n",
        "    # Remove HTML/XML tags like <e>...</e> or <a>...</a>\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Replace URLs with a generic link indicator\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "    text = re.sub(r'www\\\\.(?:[a-zA-Z]|[0-9]|[$_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '_url_', text)\n",
        "\n",
        "    # Handle *other* mentions conditionally (the first one after RT is already handled if it was a retweet)\n",
        "    if genericize_mentions_in_retweets:\n",
        "        # Replace any remaining mentions with generic _mention_ indicator\n",
        "        text = re.sub(r'@\\\\w+', '_mention_', text)\n",
        "    else:\n",
        "        # Remove any remaining mentions\n",
        "        text = re.sub(r'@\\\\w+', '', text)\n",
        "\n",
        "    # Remove non-alphanumeric characters (keeping spaces and underscore for _url_ and _mention_)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\\\s_]', '', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
        "\n",
        "    # Add the retweet prefix back\n",
        "    return (retweet_prefix + text).strip()\n",
        "\n",
        "# Time formats for parsing (from the notebook)\n",
        "time_formats = [\n",
        "    '%H:%M:%S%z',  # e.g., 10:02:57-05:00\n",
        "    '%I:%M:%S %p', # e.g., 1:22:46 PM\n",
        "    '%p %I:%M:%S', # e.g., PM 9:44:54\n",
        "    '%I:%M %p',    # e.g., 1:22 PM\n",
        "    '%H:%M',       # e.g., 10:02\n",
        "    '%p %I:%M'     # e.g., PM 9:44\n",
        "]\n",
        "\n",
        "def parse_mixed_time_formats(time_str):\n",
        "    if not isinstance(time_str, str):\n",
        "        return pd.NaT\n",
        "    time_str = time_str.strip() # Remove leading/trailing whitespace\n",
        "    for fmt in time_formats:\n",
        "        try:\n",
        "            return datetime.strptime(time_str, fmt).time()\n",
        "        except ValueError:\n",
        "            continue\n",
        "    return pd.NaT # Return NaT if no format matches\n",
        "\n",
        "# Modified train function for SVM\n",
        "def train_models(excel_file_path):\n",
        "    # Load data\n",
        "    df_obama = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "    df_romney = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "\n",
        "    df_obama = df_obama.drop(index=0).reset_index(drop=True)\n",
        "    df_romney = df_romney.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama = df_obama.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney = df_romney.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    merged_df = pd.concat([df_obama, df_romney], ignore_index=True)\n",
        "\n",
        "    columns_to_drop = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in merged_df.columns]\n",
        "    if columns_to_drop:\n",
        "        merged_df = merged_df.drop(columns=columns_to_drop)\n",
        "\n",
        "    print(\"Excel Loaded\")\n",
        "\n",
        "    # Clean tweets (using default, without generic mentions for simplicity)\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['tweet_clean'] = df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "\n",
        "    # Parse time\n",
        "    for df in [df_obama, df_romney, merged_df]:\n",
        "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "        df['time_parsed'] = df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "        df['hour_of_day'] = df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "        df['hour_of_day'] = df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Include all classes: -1, 0, 1\n",
        "    valid_classes = [-1, 0, 1]\n",
        "    df_obama = df_obama[df_obama['class'].isin(valid_classes)]\n",
        "    df_romney = df_romney[df_romney['class'].isin(valid_classes)]\n",
        "    merged_df = merged_df[merged_df['class'].isin(valid_classes)]\n",
        "\n",
        "    # Map labels to 0,1,2 for categorical: -1 -> 0 (negative), 0 -> 1 (neutral), 1 -> 2 (positive)\n",
        "    label_map = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "    # Function to prepare X and fit\n",
        "    def prepare_and_fit(df, prefix):\n",
        "        tfidf = TfidfVectorizer(max_features=10000)\n",
        "        X_text = tfidf.fit_transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_columns = hour_onehot.columns\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "\n",
        "        y = df['class'].map(label_map)\n",
        "\n",
        "        model = SVC(probability=True, kernel='linear', class_weight='balanced')\n",
        "        model.fit(X, y)\n",
        "\n",
        "        # Save\n",
        "        joblib.dump(model, f'svm_{prefix}.pkl')\n",
        "        joblib.dump(tfidf, f'tfidf_{prefix}.pkl')\n",
        "        joblib.dump(hour_columns, f'hour_columns_{prefix}.pkl')\n",
        "\n",
        "        return model, tfidf, hour_columns\n",
        "\n",
        "    # Train on Obama\n",
        "    model_obama, tfidf_obama, hour_columns_obama = prepare_and_fit(df_obama, 'obama')\n",
        "\n",
        "    # Train on Romney\n",
        "    model_romney, tfidf_romney, hour_columns_romney = prepare_and_fit(df_romney, 'romney')\n",
        "\n",
        "    # Train on merged\n",
        "    model_merged, tfidf_merged, hour_columns_merged = prepare_and_fit(merged_df, 'merged')\n",
        "\n",
        "    # Evaluate on split for merged as example\n",
        "    X_merged = hstack([tfidf_merged.transform(merged_df['tweet_clean']),\n",
        "                       csr_matrix(pd.get_dummies(merged_df['hour_of_day'], prefix='hour').values)])\n",
        "    y_merged = merged_df['class'].map(label_map)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_merged, y_merged, test_size=0.8, random_state=42, stratify=y_merged)\n",
        "\n",
        "    y_pred = model_merged.predict(X_test)\n",
        "    y_test_orig = [label_map.get(label, label) for label in y_test]  # Wait, reverse\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    y_test_orig = [reverse_map[label] for label in y_test]\n",
        "    y_pred_orig = [reverse_map[label] for label in y_pred]\n",
        "\n",
        "    print(\"Accuracy on merged test split:\", accuracy_score(y_test_orig, y_pred_orig))\n",
        "    print(classification_report(y_test_orig, y_pred_orig, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "\n",
        "    print(\"Models trained and saved.\")\n",
        "\n",
        "    return model_obama, tfidf_obama, hour_columns_obama, model_romney, tfidf_romney, hour_columns_romney, model_merged, tfidf_merged, hour_columns_merged\n",
        "\n",
        "# Modified prediction function\n",
        "def predict_on_new_data(excel_file_path):\n",
        "    # Load models, tfidfs, hour_columns\n",
        "    model_obama = joblib.load('svm_obama.pkl')\n",
        "    tfidf_obama = joblib.load('tfidf_obama.pkl')\n",
        "    hour_columns_obama = joblib.load('hour_columns_obama.pkl')\n",
        "\n",
        "    model_romney = joblib.load('svm_romney.pkl')\n",
        "    tfidf_romney = joblib.load('tfidf_romney.pkl')\n",
        "    hour_columns_romney = joblib.load('hour_columns_romney.pkl')\n",
        "\n",
        "    model_merged = joblib.load('svm_merged.pkl')\n",
        "    tfidf_merged = joblib.load('tfidf_merged.pkl')\n",
        "    hour_columns_merged = joblib.load('hour_columns_merged.pkl')\n",
        "\n",
        "    # Load new data\n",
        "    try:\n",
        "        df_obama_new = pd.read_excel(excel_file_path, sheet_name='Obama')\n",
        "        df_romney_new = pd.read_excel(excel_file_path, sheet_name='Romney')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Excel file '{excel_file_path}' not found.\")\n",
        "        return None\n",
        "    except ValueError as e:\n",
        "        print(f\"Error loading Excel sheets: {e}. Check sheet names.\")\n",
        "        return None\n",
        "\n",
        "    df_obama_new = df_obama_new.drop(index=0).reset_index(drop=True)\n",
        "    df_romney_new = df_romney_new.drop(index=0).reset_index(drop=True)\n",
        "\n",
        "    df_obama_new = df_obama_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "    df_romney_new = df_romney_new.rename(columns={'Unnamed: 4': 'class'})\n",
        "\n",
        "    new_data_df = pd.concat([df_obama_new, df_romney_new], ignore_index=True)\n",
        "\n",
        "    columns_to_drop_new = [col for col in ['Unnamed: 0', 'Unnamed: 5'] if col in new_data_df.columns]\n",
        "    if columns_to_drop_new:\n",
        "        new_data_df = new_data_df.drop(columns=columns_to_drop_new)\n",
        "\n",
        "    new_data_df['tweet_clean'] = new_data_df['Anootated tweet'].apply(clean_tweet_for_new_llm)\n",
        "\n",
        "    new_data_df['date'] = pd.to_datetime(new_data_df['date'], errors='coerce')\n",
        "    new_data_df['time_parsed'] = new_data_df['time'].astype(str).apply(parse_mixed_time_formats)\n",
        "    new_data_df['hour_of_day'] = new_data_df['time_parsed'].apply(lambda x: x.hour if pd.notna(x) else 0)\n",
        "    new_data_df['hour_of_day'] = new_data_df['hour_of_day'].astype(int)\n",
        "\n",
        "    # Function to prepare X for prediction\n",
        "    def prepare_X_new(df, tfidf, hour_columns):\n",
        "        X_text = tfidf.transform(df['tweet_clean'])\n",
        "\n",
        "        hour_onehot = pd.get_dummies(df['hour_of_day'], prefix='hour')\n",
        "        hour_onehot = hour_onehot.reindex(columns=hour_columns, fill_value=0)\n",
        "        X_hour = csr_matrix(hour_onehot.values)\n",
        "\n",
        "        X = hstack([X_text, X_hour])\n",
        "        return X\n",
        "\n",
        "    # Get probas\n",
        "    X_obama_new = prepare_X_new(new_data_df, tfidf_obama, hour_columns_obama)\n",
        "    proba_obama = model_obama.predict_proba(X_obama_new)\n",
        "    class_obama = np.argmax(proba_obama, axis=1)\n",
        "    max_proba_obama = np.max(proba_obama, axis=1)\n",
        "\n",
        "    X_romney_new = prepare_X_new(new_data_df, tfidf_romney, hour_columns_romney)\n",
        "    proba_romney = model_romney.predict_proba(X_romney_new)\n",
        "    class_romney = np.argmax(proba_romney, axis=1)\n",
        "    max_proba_romney = np.max(proba_romney, axis=1)\n",
        "\n",
        "    X_merged_new = prepare_X_new(new_data_df, tfidf_merged, hour_columns_merged)\n",
        "    proba_merged = model_merged.predict_proba(X_merged_new)\n",
        "    class_merged = np.argmax(proba_merged, axis=1)\n",
        "    max_proba_merged = np.max(proba_merged, axis=1)\n",
        "\n",
        "    # Select the class with the highest probability across models\n",
        "    pred_classes = []\n",
        "    for i in range(len(new_data_df)):\n",
        "        probas = [max_proba_obama[i], max_proba_romney[i], max_proba_merged[i]]\n",
        "        classes = [class_obama[i], class_romney[i], class_merged[i]]\n",
        "        max_idx = np.argmax(probas)\n",
        "        pred_classes.append(classes[max_idx])\n",
        "\n",
        "    reverse_map = {0: -1, 1: 0, 2: 1}\n",
        "    new_data_df['pred_class'] = [reverse_map[c] for c in pred_classes]\n",
        "\n",
        "    return new_data_df\n",
        "\n",
        "# Usage example (adapt paths as needed)\n",
        "excel_file_path = '/content/drive/MyDrive/cs583/training-Obama-Romney-tweets.xlsx'\n",
        "train_models(excel_file_path)\n",
        "predicted_df = predict_on_new_data(excel_file_path)\n",
        "if predicted_df is not None:\n",
        "    print(predicted_df[['Anootated tweet', 'pred_class']].head())\n",
        "    # Metrics if 'class' is available\n",
        "\n",
        "    # Filter predicted_df to only include valid classes for y_true\n",
        "    valid_classes_for_report = [-1, 0, 1]\n",
        "    filtered_predicted_df = predicted_df[predicted_df['class'].isin(valid_classes_for_report)].copy()\n",
        "\n",
        "    y_true = pd.to_numeric(filtered_predicted_df['class'], errors='coerce').dropna()\n",
        "    y_pred = pd.to_numeric(filtered_predicted_df['pred_class'], errors='coerce').dropna()\n",
        "    common_indices = y_true.index.intersection(y_pred.index)\n",
        "    y_true = y_true.loc[common_indices]\n",
        "    y_pred = y_pred.loc[common_indices]\n",
        "    if not y_true.empty and not y_pred.empty:\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        report = classification_report(y_true, y_pred, target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)'], zero_division=0)\n",
        "        print(\"\\n--- Prediction Metrics ---\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "\n",
        "# Generate the .txt output file\n",
        "output_filename = 'predictionsSVM.txt'\n",
        "with open(output_filename, 'w') as f:\n",
        "    f.write('(setf x *(\\n')\n",
        "    for original_index, row in predicted_df.iterrows():\n",
        "        # The tweet number here must correspond to the line/tweet number in each Excel file.\n",
        "        # We use original_index + 1 to get 1-based indexing for tweet number\n",
        "        f.write(f'  ({original_index + 1} {row[\"pred_class\"]})\\n')\n",
        "    f.write(') )\\n')\n",
        "print(f\"Predictions saved to {output_filename} in the specified format.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODq_zhkyZrjg",
        "outputId": "ee84cfab-6ad4-403e-c974-b15e9ad90d3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Excel Loaded\n",
            "Accuracy on merged test split: 0.9463803956834532\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.95      0.96      0.95      3852\n",
            "  Neutral (0)       0.95      0.93      0.94      2861\n",
            " Positive (1)       0.93      0.95      0.94      2183\n",
            "\n",
            "     accuracy                           0.95      8896\n",
            "    macro avg       0.94      0.95      0.94      8896\n",
            " weighted avg       0.95      0.95      0.95      8896\n",
            "\n",
            "Models trained and saved.\n",
            "                                     Anootated tweet  pred_class\n",
            "0  Kirkpatrick, who wore a baseball cap embroider...           0\n",
            "1  Question: If <e>Romney</e> and <e>Obama</e> ha...          -1\n",
            "2  #<e>obama</e> debates that Cracker Ass Cracker...           1\n",
            "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...          -1\n",
            "4  @Hollivan @hereistheanswer  Youre missing the ...           0\n",
            "\n",
            "--- Prediction Metrics ---\n",
            "Accuracy: 0.9471\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.90      1.00      0.94      4815\n",
            "  Neutral (0)       1.00      0.85      0.92      3576\n",
            " Positive (1)       0.99      0.99      0.99      2728\n",
            "\n",
            "     accuracy                           0.95     11119\n",
            "    macro avg       0.96      0.94      0.95     11119\n",
            " weighted avg       0.95      0.95      0.95     11119\n",
            "\n",
            "Predictions saved to predictionsSVM.txt in the specified format.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results\n",
        "\n",
        "\n",
        "### process\n",
        "\n",
        "Feature Vectorization:\n",
        "\n",
        "\n",
        "Text features ('tweet_clean') are transformed using TfidfVectorizer (max_features=10000), which converts text to TF-IDF vectors (term frequency-inverse document frequency, emphasizing important words while downweighting common ones).\n",
        "Hour features are one-hot encoded into a sparse matrix.\n",
        "The final feature matrix X is a horizontal stack (hstack) of TF-IDF text features and one-hot hour features. This combines textual and temporal information.\n",
        "\n",
        "\n",
        "For K=3 classes (negative=0, neutral=1, positive=2), OvO trains $\\binom{K}{2} = \\frac{K(K-1)}{2} = 3$ binary SVM classifiers:\n",
        "\n",
        "Classifier 1: Negative (0) vs. Neutral (1)\n",
        "Trained only on samples where y=0 or y=1.\n",
        "Finds a hyperplane separating negative from neutral tweets.\n",
        "\n",
        "Classifier 2: Negative (0) vs. Positive (2)\n",
        "Trained only on samples where y=0 or y=2.\n",
        "Separates negative from positive.\n",
        "\n",
        "Classifier 3: Neutral (1) vs. Positive (2)\n",
        "Trained only on samples where y=1 or y=2.\n",
        "Separates neutral from positive.\n",
        "\n",
        "\n",
        "\n",
        "For each model (Obama, Romney, merged), predict_proba(X_new) returns probabilities for each class (shape: [n_samples, 3]).\n",
        "Internally, for OvO:\n",
        "Each of the 3 binary classifiers outputs a decision score.\n",
        "Voting: The class with the most \"wins\" (e.g., if classifier 1 picks 0 over 1, classifier 2 picks 0 over 2, then 0 wins 2 votes).\n",
        "Ties broken by distance to hyperplane or fallback rules.\n",
        "Probabilities (via probability=True): Post-processes votes/scores with sigmoid calibration to get [P(class=0), P(class=1), P(class=2)].\n",
        "\n",
        "class_model = np.argmax(proba_model, axis=1): Selects the class with highest prob per model.\n",
        "max_proba_model = np.max(proba_model, axis=1): Highest probability value for that class."
      ],
      "metadata": {
        "id": "PigXGBE8dGY9"
      }
    }
  ]
}